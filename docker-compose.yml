version: "3.9"

x-airflow-common:
  &airflow-common
  build:
    context: ./airflow
    dockerfile: Dockerfile
  env_file:
    - .env
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
  depends_on:
    - postgres

services:
  postgres:
    image: postgres:15
    env_file:
      - .env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5433:5432"
    volumes:
      - ./postgres_data:/var/lib/postgresql/data

  #airflow-init:
   # <<: *airflow-common
    #entrypoint: /bin/bash
    #command:
    #  - -c
     # - |
      #  airflow db init &&
       # airflow users create \
        #  --username admin \
         # --password admin \
          #--firstname Admin \
          #--lastname User \
          #--role Admin \
          #--email admin@example.com
    #restart: "no"

  #airflow-webserver:
   # <<: *airflow-common
    #command: webserver
    #ports:
     # - "8080:8080"
    #restart: always

  #airflow-scheduler:
   # <<: *airflow-common
    #command: scheduler
    #restart: always

  
  spark-master:
    build: ./spark
    command: /usr/local/spark-3.4.1-bin-hadoop3/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "7077:7077"    
      - "8081:8080"   
    environment:
    - SPARK_MODE=master 
    volumes:
    - ./spark/scripts:/opt/scripts 
    mem_limit: "1g"
    cpus: 0.5

  spark-worker:
    build: ./spark
    command: /usr/local/spark-3.4.1-bin-hadoop3/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    volumes:
    - ./spark/scripts:/opt/scripts 
    mem_limit: "1g"
    cpus: 0.5
    depends_on:
      - spark-master