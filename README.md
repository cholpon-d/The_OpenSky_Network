# âœˆï¸ The OpenSky Network â€” Data Pipeline with Airflow, Spark & ClickHouse

> End-to-end **ETL pipeline** that collects, processes, and stores real-time flight data  
> using **Apache Airflow**, **PostgreSQL**, **Apache Spark**, and **ClickHouse** â€” all inside **Docker** ğŸš€

---

### ğŸ›°ï¸ Data Flow

**OpenSky API â†’ ğŸª¶ Airflow â†’ ğŸ˜ PostgreSQL â†’ âš¡ Spark â†’ ğŸ­ ClickHouse**

| Component             | Role                                            |
| --------------------- | ----------------------------------------------- |
| ğŸª¶ **Airflow**        | Orchestrates DAGs for API ingestion & data flow |
| âš¡ **Spark**          | Transforms and processes flight data            |
| ğŸ­ **ClickHouse**     | Stores processed data for analytics             |
| ğŸ˜ **PostgreSQL**     | Used by Airflow as metadata & staging DB        |
| ğŸ³ **Docker Compose** | Manages the entire local environment            |
| ğŸ **Python 3.10+**   | Language                                        |

---

## ğŸ§± Project Structure

```bash
.
â”œâ”€â”€ dags/                          # Airflow DAGs: orchestrate data workflow
â”‚   â”œâ”€â”€ create_clickhouse_tables.py
â”‚   â”œâ”€â”€ spark_to_clickhouse.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ Dockerfile                 # Spark master & worker image
â”‚   â”œâ”€â”€ spark_to_clickhouse.py     # Spark transformation script
â”‚   â””â”€â”€ work/                      # temp folder (gitignored)
â”‚
â”œâ”€â”€ shared_data/                   # shared volume between Airflow & Spark
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## âš™ï¸ Prerequisites

Before running the project, make sure you have:

ğŸ³ Docker

ğŸ§© Docker Compose

ğŸ’¾ At least 8 GB RAM

## ğŸ—ï¸ Setup & Run

1ï¸âƒ£ Build and start all containers

`docker-compose up -d --build`

2ï¸âƒ£ Initialize Airflow

`docker-compose run airflow-init`

Then restart all services:

`docker-compose up -d`

3ï¸âƒ£ Verify containers are running

`docker ps`

## ğŸŒ Web Interfaces

Service URL Description

ğŸª¶ Airflow Web UI http://localhost:8082 DAG management & monitoring

âš¡ Spark Master UI http://localhost:8080 Spark jobs & worker status

ğŸ­ ClickHouse UI http://localhost:8123 Query processed data

ğŸ˜ PostgreSQL Port 5433 Metadata database for Airflow

## âœˆï¸ Workflow Summary

ğŸ“¡ Fetch flight data from the OpenSky API

ğŸ“¥ Load raw flight data into PostgreSQL for persistence

ğŸ• Wait for ClickHouse tables to be available

âš¡ Trigger a Spark job inside the container for transformation

ğŸ’¾ Store processed data into ClickHouse for analytics

ğŸ§  Log every stage for better observability

## âš™ï¸ Environment Variables

All required environment variables are defined in `.env.example.`

Simply copy and rename it before starting the containers:

`cp .env.example .env`

No manual exports required â€” Docker Compose automatically loads variables from .env.

## ğŸ§¹ Cleanup

Stop and remove everything (including volumes):

`docker-compose down -v`

## ğŸ§  Notes

ğŸ—‚ï¸ shared_data/ and spark/work/ are temporary folders (already in .gitignore)

âš™ï¸ Health checks are disabled for better stability on low-memory systems

ğŸ§¾ To view container logs:

`docker logs airflow-webserver | tail -n 50`

â­ Enjoy exploring flight data pipelines powered by Airflow, Spark, and ClickHouse!
